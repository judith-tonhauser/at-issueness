% !TEX TS-program = lualatexmk
% glossa-template.tex
% Copyright 2016 Guido Vanden Wyngaerd
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
% 
% The Current Maintainer of this work is 
% Guido Vanden Wyngaerd (guido.vandenwyngaerd@kuleuven.be).
%
% This work consists of the files 
% glossa.cls
% glossa.bst
% gl-authoryear-comp.cbx
% biblatex-gl.bbx
% glossa-template.tex
% glossa.png
%
% The files of the work are derived from the Semantics & Pragmatics style files
% by Kai von Fintel, Christopher Potts, and Chung-chieh Shan
% All changes are documented on the github repository 
% https://github.com/guidovw/Glossalatex.

\PassOptionsToPackage{table}{xcolor}
\PassOptionsToPackage{xcolor}{dvipsnames}
\documentclass[times,linguex]{glossa}
\usepackage{rotating}
\usepackage{tablefootnote}
\usepackage{colortbl}
\usepackage{color}
\usepackage{multicol}
\usepackage{booktabs}

\usepackage{adjustbox}
\usepackage{array}

\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}

\newcommand*\rots{\multicolumn{1}{R{90}{.7em}}}% no optional argument here, please!
%\usepackage{xcolor}

% possible options:
% [times] for Times font (default if no option is chosen)
% [cm] for Computer Modern font
% [lucida] for Lucida font (not freely available)
% [brill] open type font, freely downloadable for non-commercial use from http://www.brill.com/about/brill-fonts; requires xetex
% [charis] for CharisSIL font, freely downloadable from http://software.sil.org/charis/
% for the Brill an CharisSIL fonts, you have to use the XeLatex typesetting engine (not pdfLatex)
% for headings, tables, captions, etc., Fira Sans is used: https://www.fontsquirrel.com/fonts/fira-sans
% [biblatex] for using biblatex (the default is natbib, do not load the natbib package in this file, it is loaded automatically via the document class glossa.cls)
% [linguex] loads the linguex example package
% !! a note on the use of linguex: in glossed examples, the third line of the example (the translation) needs to be prefixed with \glt. This is to allow a first line with the name of the language and the source of the example. See example (2) in the text for an illustration.
% !! a note on the use of bibtex: for PhD dissertations to typeset correctly in the references list, the Address field needs to contain the city (for US cities in the format "Santa Cruz, CA")

%\addbibresource{sample.bib}
% the above line is for use with biblatex
% replace this by the name of your bib-file (extension .bib is required)
% comment out if you use natbib/bibtex

\let\B\relax %to resolve a conflict in the definition of these commands between xyling and xunicode (the latter called by fontspec, called by charis)
\let\T\relax
\usepackage{xyling} %for trees; the use of xyling with the CharisSIL font produces poor results in the branches. This problem does not arise with the packages qtree or forest.
%\usepackage[linguistics]{forest} %for nice trees!


% \pdf* commands provide metadata for the PDF output. ASCII characters only!
\pdfauthor{}
\pdftitle{What is at-issueness?}
\pdfkeywords{}

\title[What is at-issueness?]{What is at-issueness? An experimental comparison of diagnostics\\ 
  % \bigskip \large Word count: 4720
  }
% Optional short title inside square brackets, for the running headers.

\author[]% short form of the author names for the running header. If no short author is given, no authors print in the headers.
{%as many authors as you like, each separated by \AND.
  % \spauthor{Waltraud Paul\\
  % \institute{CRLAO, CNRS-EHESS-INALCO}\\
  % \small{%105, Bd. Raspail, 75005 Paris\\
  % waltraud.paul@ehess.fr}
  % }
  % \AND
  % \spauthor{Guido Vanden Wyngaerd \\
  % \institute{KU Leuven}\\
  % \small{%Warmoesberg 26, 1000 Brussel\\
  % guido.vandenwyngaerd@kuleuven.be}
  % }%
}

\input{author-added}

% positive coefficients/difference
\definecolor{red}{RGB}{178,24,43}

% negative coefficients/difference
\definecolor{blue}{RGB}{33,102,172}

% comments by JT
\newcommand{\jt}[1]{\textbf{\color{orange}JT: #1}}
\newcommand{\lh}[1]{\textbf{\color{Cerulean}LH: #1}}

\begin{document}


\maketitle


\begin{abstract}
  At-issueness is a key concept in theoretical semantics and pragmatics, but there is no consensus about how it should be defined or diagnosed (e.g., \citealt{tonhauser_diagnosing_2012,snider_anaphoric_2017,snider_distinguishing_2018,tonhauser_how_2018,koev_notions_2018,faller_discourse_2019,korotkova_evidential_2020}). We present experimental data from a series of six experiments, investigating whether four widely used diagnostics for at-issueness yield consistent results. Our findings reveal substantial differences across diagnostics, indicating they are not interchangeable. Our study also susggests that diagnostics that assess the at-issueness of contents embedded in questions show the greatest variability across expressions, suggesting that speech act embedding plays a key role. However, our results are inconclusive as to whether some differences between diagnostics arise from distinct response tasks, different underlying notions of at-issueness, or other discourse factors.

\end{abstract}

% \begin{keywords}
%   at-issueness, experimental pragmatics, discourse interpretation
% \end{keywords}

\section{Introduction \label{sec:1_introduction}}
  
  At-issueness is a key concept in theoretical semantics and pragmatics, used in the analysis of a range of phenomena, including presupposition, conventional implicature, evidentials, and expressives (e.g., \citealt{karttunen_conventional_1979,horton_presuppositions_1988,abbott_presuppositions_2000,faller_semantics_2003,potts_logic_2005,tonhauser_diagnosing_2012}). Although it is generally understood as distinguishing propositions expressing the main point of an utterance (at-issue content) from those conveying background information (not-at-issue content), there is no consensus on how to define this distinction, and multiple competing definitions and diagnostics reflect different assumptions about its underlying nature.

  Four commonly used diagnostics are illustrated in (\pref{qud}--\pref{yesbut}) for sentence-medial appositive non-restrictive relative clauses (NRRCs), which are typically taken to contribute not-at-issue content (\citealt{potts_logic_2005}). Accordingly, participants are expected to: give low question-answer-match ratings under the QUD diagnostic \ref{qud}; judge that the speaker is not asking about the  NRRC content under the `asking-whether' diagnostic \ref{aw}; give low naturalness ratings under the direct-dissent diagnostic \ref{dd}; and prefer a \emph{yes}-response under the `yes, but' diagnostic in \ref{yesbut}.

  \ex. \label{qud}%
    QUD diagnostic (e.g., \citealt{tonhauser_diagnosing_2012,chen_presuppositions_2024})
    \a.[A:] \emph{What did Greg buy?}
    \b.[B:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    \z.
    Question to participants: How well does B's response fit A's question?
  \z.

  \ex. \label{aw}%
    `asking whether' diagnostic (e.g., \citealt{tonhauser_how_2018,solstad_cataphoric_2024})\smallskip\\
      \emph{Is Greg, who bought a new car, envied by his neighbor?}\smallskip
  \\ Question to participants: Is the speaker asking whether Greg bought a new car?
  \z.

  \ex. \label{dd} Direct-dissent diagnostic (e.g., \citealt{tonhauser_diagnosing_2012,syrett_experimental_2015})
    \a.[A:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    \b.[B:]\emph{No, that's not true, he didn't buy a new car.}
    \z.
  Question to participants: How natural is B's rejection of A's utterance?
  \z.

  \ex. \label{yesbut}%
    `yes, but' diagnostic (e.g., \citealt{xue_correlation_2011,destruel_cross-linguistic_2015})
    \a.[A:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    \b.[B:] \emph{Yes, but he didn't buy a new car.} /
    \b.[] \emph{Yes, and he didn't buy a new car.} /
    \b.[] \emph{No, he didn't buy a new car.}
    \z.
    Task for participants: Choose the response that sounds best.
  \z.

  The QUD diagnostic \ref{qud} and the `asking whether' diagnostic \ref{aw} assume that discourse is structured around addressing a Question Under Discussion (QUD) (\citealt{roberts_information_1996,ginzburg_interrogatives_1996}), and that the at-issue content of an utterance interacts with the QUD (\citealt{amaral_review_2007,simons_what_2010}). Specifically, the QUD diagnostic (\citealt{tonhauser_diagnosing_2012,chen_presuppositions_2024}) assumes that only at-issue content can address a previously established QUD, while the `asking whether' diagnostic (\citealt{tonhauser_how_2018,solstad_cataphoric_2024,degen-tonhauser-glossa}) assumes that the at-issue content of a question explicitly raises a QUD, whereas its not-at-issue content does not contribute to determining that QUD.

  In contrast, the direct-dissent diagnostic \ref{dd} and the `yes but' diagnostic \ref{yesbut} are usually not interpreted within a QUD-framework (\citealt{snider_at-issueness_2017,snider_anaphoric_2017,snider_distinguishing_2018,koev_notions_2018,faller_discourse_2019,korotkova_evidential_2020}). Previous research has emphazised that these assent/dissent-based diagnostics are based on different assumptions: \citet{snider_anaphoric_2017,snider_at-issueness_2017,snider_distinguishing_2018} has argued that they may not reflect at-issueness at all, but rather target the anaphoric availability of propositional content for response particles like English \emph{yes/no}. On another view, these diagnostics target a distinct notion of at-issueness (see \citealt{koev_notions_2018} and discussion in \citealt{faller_discourse_2019,korotkova_evidential_2020}), under which the at-issue content of an assertion constitues a proposal to update the common ground (based on \citealt{farkas_reacting_2010}). Accordingly, such content can be directly affirmed or denied using default discourse moves that include response particles. Not-at-issue content is either presupposed (already entailed in the common ground; \citealt{stalnaker_presuppositions_1973,stalnaker_common_2002}), or newly imposed on the common ground (\citealt{murray_varieties_2014,anderbois_at-issue_2015}), and requires special discourse moves for rejection (\citealt{potts_logic_2005}).

  Consequently, there is currently no agreed-upon definition of at-issueness, and existing diagnostics may not be probing the same underlying phenomenon. This raises two central questions:

  \begin{enumerate}
    \item Do the various diagnostics yield the same results?
    \item Do the existing definitions of at-issueness describe the same underlying phenomenon?
  \end{enumerate}

  \noindent Initial research on these questions has yielded mixed answers.

  While adnominal appositives are often reported to contribute not-at-issue content (\citealt{potts_logic_2005,amaral_review_2007,tonhauser_diagnosing_2012,destruel_cross-linguistic_2015,tonhauser_how_2018,solstad_cataphoric_2024}), a forced-choice continuation task in \citet{syrett_experimental_2015} suggested that sentence-final appositive NRRCs are more at-issue than medial ones under a version of the direct-dissent diagnostic (see also \citealt{anderbois_crossing_2010}). This finding has been interpreted as evidence that final apposive NRRCs are more at-issue than medial ones at the point in discourse when participants select a response (\citealt{syrett_experimental_2015,jasinskaja_not_2016}). \citealt{koev_notions_2018} further argued that the direct-dissent diagnostic is sensitive to this medial/final distinction, while other tests are not.

  Beyond appositives, \citealt{tonhauser_how_2018} compared at-issueness measures for a range of 19 English expressions including sentence-medial appositives and the clause-embedding predicates \emph{know, discover} and \emph{annoyed}, using two diagnostics: the question-based `asking whether' diagnostic and the `Are you sure?' diagnostic (p.526ff). They found that contents generally received lower at-issueness ratings with the `asking whether' diagnostic, while the `Are you sure?' diagnostic showed greater variability across expressions. Despite these differences, ratings from both diagnostics correlated with measures of projection inferences, suggesting that they may nonetheless be sensitive to a shared underlying phenomenon.

  To date, however, there has been no systematic experimental comparison of diagnostics to investigate whether the diagnostics yield consistent results. This paper takes a first step toward addressing the above questions (i+ii) in a systematic way. Specifically, we assess whether the four widely used diagnostics in (\pref{qud}--\pref{yesbut}) give rise to the same pattern of results when applied to the same target contents. Our findings reveal significant differences across diagnostics, indicating they are not interchangeable. Because each diagnostic is grounded in distinct theoretical assumptions about what it means for content to be at-issue, this comparison also bears on whether currently available conceptions of at-issueness converge on a common underlying notion.

  We present the results from a series of experiments that systematically vary the diagnostic used to assess the at-issueness status of the same types of contents across experiments in English. Experiments 1--4 tests propositional contents associated with seven types of expressions: sentence-medial and sentence-final NRRCs and the complements of selected clause-embedding predicates (\emph{be right, confirm, confess, discover,} and \emph{know}). These contents were selected because previous research has found variability in their behavior on at-issueness diagnostics, such as Syrett and Koev's finding that medial and final NRRCs yield different results on the direct dissent diagnostic. Further, \citealt{degen-tonhauser-glossa} found fine-grained differences between the embedded content of 20 English clause-embedding predicates using the asking-whether diagnostic. \Cref{fig:dtglossa} shows the mean `asking whether' ratings by predicate in their study.

  \begin{figure}[h!]
    \centering

    \includegraphics[width=0.7\textwidth]{../../results+analysis/degen-tonhauser-glossa/graphs/mean-asking-whether-ratings.pdf}

    \caption{Mean `asking whether' ratings for the contents of the clausal complements of 20 clause-embedding predicates, from \citealt{degen-tonhauser-glossa}. Error bars indicate 95\% bootstrapped confidence intervals. Violin plots indicate the distribution of individual ratings.
    }

    
    \label{fig:dtglossa}
  \end{figure}

  By directly comparing how each diagnostic evaluates these contents are evaluated, our study provides the first systematic assessment of the comparability and interchangeability of at-issueness diagnostics. Experiments 1--4 used the four diagnostics in \ref{qud}--\ref{yesbut} to assess at-issueness for contents expressed by NRRCs and the complement of our selected clause-embedding predicates. None of the experiments observed the positional effect found by \citet{syrett_experimental_2015}, suggesting that even small task differences can affect outcomes. Further, only Exp.~2 (`asking whether') comes close to observing the differences between the clause-embedding predicates observed in \citet{degen-tonhauser-glossa}, suggesting that diagnostics do vary in whether they detect at-issueness differences between particular expressions. The `asking whether' diagnostic, as implemented in Exp.~2, showed the greatest differentiation between contents, and it is the only diagnostic where the target content is embedded in a polar question. To test whether the greater differentiation stems from interrogative embedding or from the response task itself, we compared the `asking whether' diagnostic (Exp.~5) with a direct-response diagnostic that also embeds content in a polar question (Exp.~6), for the 20 clause-embedding predicates from \citet{degen-tonhauser-glossa}. The two experiments yielded highly correlated results, suggesting that the observed differentiation is related more to the interrogative embedding, not the response task.

  Our findings suggest that the four diagnostics are not interchangeable, because they differ in the relative differentiation of the contents investigated. Our findings are not conlcusive regarding whether (some of) these differences arise from different response tasks, different underlying notions of at-issueness, or other discourse factors. However, we find evidence that speech act in which the target content is presented makes a difference. A takeaway is that future empirical investigations of at-issueness should continue the path of using multiple diagnostics to ensure that results are not specific to a particular diagnostic. Further, because the interrogative embedding appears to matter, the role of speech act type should be considered when diagnosing at-issueness.

  We operationalize each diagnostic through its established empirical task: question-answer match ratings for the QUD diagnostic, speaker intention judgments for the asking-whether diagnostic, naturalness ratings for direct dissent, and forced-choice responses for the `yes, but' diagnostic.
  
  For each of the contents, we collect ratings across multiple items and participants, and compare the mean responses. Given that we are aggregating over multiple items and participants, we may say that the mean rating for one content is higher/lower than that of another, or that they do not differ. Following prior work (e.g., \citealt{tonhauser_how_2018}), we interpret higher mean/lower ratings as indicating that content is more/less at-issue under a given diagnostic, with two caveats:

  First, we remain agnostic about whether at-issueness is an underlyingly binary or a gradient property (cf. \citealt{tonhauser_how_2018,barnes_information_2023}). If at-issueness is gradient, the extent to which a content is at-issue may be understood as the extent to which it is relevant to the QUD or the main assertion, in which case gradient mean ratings may be taken to reflect gradient relevance. If at-issueness is categorical, content is at-issue iff it addresses the QUD or assertive proposal, and not-at-issue otherwise; and gradient mean ratings could be attributed to uncertainty about what the QUD is. For example, our interpretation of a content in a given utterance being more/less at-issue may be interpreted as reflecting the frequency or ease with which a particular QUD is attributed to that utterance.
  %
  Second, we use the term \enquote{at-issueness diagnostic} descriptively throughout the paper, even though our findings may ultimately suggest that these diagnostics track distinct theoretical constructs. We return to this issue in the general discussion.



\section{Experiments 1-4 \label{sec:2_experiments}}

  % 
    To compare the results of at-issueness diagnostics, we conducted four experiments that each measured at-issueness with a different diagnostic, namely the QUD diagnostic (Exp.~1), the `asking whether' diagnostic (Exp.~2), the direct-dissent diagnostic (Exp.~3) and the `yes, but' diagnostic (Exp.~4).\footnote{The experiments, data and R code for generating the figures and analyses of the experiments reported in this paper are available at INSERT URL TO ANONYMOUS GITHUB REPO BEFORE SUBMISSION. All experiments were conducted with approval from the ethics review committee of [university name redacted for review]. \label{f:github}}
    %    
    Each experiment tested the same manipulation, comparing the propositional contents of the seven types of expressions (contents) illustrated in \ref{stims}: the contents of sentence-medial and sentence-final NRRCs \ref{stims.a}-\ref{stims.b}, as well as the contents of the clausal complements of \emph{know, discover, confess, confirm} and \emph{be right} \ref{stims.c}-\ref{stims.g}. Contents were randomly paired with items from a set of items shared across all four experiments (see example pairings in \Next).

    \ex.\label{stims}
    \a.\label{stims.a} Content of sentence-medial NRRC \\
      \emph{Lucy, who broke the plate, apologised.} $\leadsto$ Lucy broke the plate\smallskip
    \b.\label{stims.b} Content of sentence-final NRRC \\
    \emph{The police found Jack, who saw the murder.} $\leadsto$ Jack saw the murder\smallskip
    \b.\label{stims.c} Content of the clausal complement of \emph{know} \\
    \emph{Ann knows that Raul cheated on his wife.} $\leadsto$ Raul cheated on his wife\smallskip
    \b.\label{stims.d} Content of the clausal complement of \emph{discover} \\
    \emph{Mary discovered that Denny ate the last cupcake.} $\leadsto$ Denny ate the last cupcake\smallskip
    \b.\label{stims.e} Content of the clausal complement of \emph{be right} \\
    \emph{Tom is right that Ann stole the money.} $\leadsto$ Ann stole the money\smallskip
    \b.\label{stims.f} Content of the clausal complement of \emph{confirm} \\
    \emph{Harry confirmed that Greg bought a new car.} $\leadsto$ Greg bought a new car\smallskip
    \b.\label{stims.g} Content of the clausal complement of \emph{confess}  \\
    \emph{Lucy confessed that Dustin lost his key.} $\leadsto$ Dustin lost his keys
    \z.
    \z.


    These seven contents were chosen because prior literature observed differences in at-issueness between two or more of these contents using a particular diagnostic for at-issueness.  Specifically, as discussed in \S1, \citealt{syrett_experimental_2015} observed differences between sentence-medial and -final NRRCs using a variant of the direct-dissent diagnostic,
    % \citealt{tonhauser_how_2018} observed differences between sentence-medial NRRCs and the contents of the complements of \emph{know, discover} and \emph{be annoyed} using the `asking whether' diagnostic,
    and \citealt{degen-tonhauser-glossa} observed differences between \emph{know, discover, confess, confirm} and \emph{be right} using the `asking whether' diagnostic. Thus, comparing these seven contents across the four diagnostics in Exps.~1-4 will allow us to assess whether the differences that emerge from one diagnostic also emerge from others. In each experiment, participants read the stimuli and gave ratings corresponding to the diagnostics.

  \subsection{Methods}
    
    \subsubsection{Participants}

      For each of the four experiments, we recruited 80 unique participants on Prolific. These participants had registered on the platform as living in the USA and as having English as their primary language. They had at least 50 previous submissions and an approval rate of at least 97\%.  Table \ref{t:recruited} shows the age and gender distributions of the recruited participants.

      \begin{table}[h!]
      \centering
      \begin{tabular}{l | c | r r r }
                  & recruited & ages (mean age) & f/m/nb/dnd \\ \hline
      Exp.~1 (QUD) & 80 & 18-81 (43.8) & 42/37/0/1  \\
      Exp.~2 (asking whether) & 80 & 20-74 (38.5)  & 48/30/1/1  \\
      Exp.~3 (direct dissent) & 80 & 18-77 (39.1) & 50/28/1/1  \\
      Exp.~4 (yes, but) &80 & 19-67 (38.0)  & 48/30/2/0 &  \\
      \hline
      \end{tabular}

      \caption{Information about the participants recruited in Exps.~1-4 (f = female, m = male, nb = nonbinary, dnd = did not disclose).}\label{t:recruited}
      \end{table}

    \subsubsection{Materials and procedure}
      
      The four experiments measured the at-issueness of the seven contents in \ref{stims}, each using a different diagnostic: the QUD diagnostic (Exp.~1), the `asking whether' diagnostic (Exp.~2), the direct-dissent diagnostic (Exp.~3), and the `yes, but' diagnostic (Exp.~4). \ref{diag} illustrates how each diagnostic was implemented using sentence-medial NRRCs (with the item `Lucy broke the plate').

      In Exp.~1 (QUD diagnostic, \ref{diag.a}), participants read a dialogue between two named speakers, where the first utters a constituent question (the presumed QUD) that is about the target content and the second responds with a declarative sentence that contributes the target content. In Exp.~2 (`asking whether' diagnostic, \ref{diag.b}), participants read a polar question uttered by a named speaker, which itself contributes the target content.
      %
      In Exp.~3 (direct-dissent diagnostic, \ref{diag.c}), participants read a dialogue between two named speakers, where the first utters a declarative sentence with the target content and the second directly dissents with the target content. 
      %
      Finally, in Exp.~4 (`yes, but' diagnostic, \ref{diag.d}), participants read a dialogue between two named speakers where the first utters a declarative sentence that contributes the target content and the second responds with one of two indirect dissent variants (\emph{yes, but..}, \emph{yes, and...}) or a direct dissent.

      \ex.\label{diag} Implementation of the diagnostics in Exps.~1-4
      \a.\label{diag.a} Exp.~1 (QUD diagnostic)
      \\ {\bf Nora:} \emph{What did Lucy break?}
      \\ {\bf Leo:} \emph{Lucy, who broke the plate, apologized.}\smallskip
      \b.\label{diag.b} Exp.~2 (`asking whether' diagnostic )
      \\ {\bf Nora:} \emph{Did Lucy, who broke the plate, apologize?}\smallskip
      \c.\label{diag.c} Exp.~3 (direct-dissent diagnostic)
      \\ {\bf Nora:} \emph{Lucy, who broke the plate, apologized.}
      \\ {\bf Leo:} \emph{No, she didn't break the plate.}\smallskip
      \d.\label{diag.d} Exp.~4 (`yes, but' diagnostic)
      \\ {\bf Nora:} \emph{Lucy, who broke the plate, apologized.}
      \\ {\bf Nina:} \emph{Yes, but she didn't break the plate.}
      \\ \hspace*{1cm} \emph{Yes, and she didn't break the plate.}
      \\ \hspace*{1cm} \emph{No, she didn't break the plate.}


      As shown in Fig.~\ref{fig:trials}, the response options differed by diagnostic. In Exp.~1 (QUD diagnostic, panel (a)), participants rated how well the response fit the question on a slider marked `totally doesn't fit' on one end (coded 0) and `totally fits' on the other end (coded as 1). In Exp.~2 (`asking whether' diagnostic, panel (b)), participants judged whether the question was about the target content, using a slider marked `no' on one end (coded as 0) and `yes' on the other (coded as 1). In Exp.~3 (direct-dissent diagnostic, panel (c)), participants rated the naturalness or the direct dissent on a slider marked `totally unnatural' (coded as 0) on one end and `totally natural' on the other (coded as 1). Finally, in Exp.~4 (`yes, but' diagnostic, panel (d)), participants chose the response that sounded best; the two indirect dissents were coded as 0 and the direct one as 1. Across the four experiments, the responses were coded so that 1 meant that the content to be diagnosed was rated as at-issue and 0 as not-at-issue.

      \begin{figure}[h!]
      \centering
      % Top row
      \subfigure[Exp.~1: QUD diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp1}}%
        \label{fig:trialExp1}
      }
      \hfill
      \subfigure[Exp.~2: `asking whether' diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp2}}%
        \label{fig:trialExp2}
      }

      \vspace{1em}

      % Bottom row
      \subfigure[Exp.~3: direct-dissent diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp3}}%
        \label{fig:trialExp3}
      }
      \hfill
      \subfigure[Exp.~4: `yes, but' diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp4}}%
        \label{fig:trialExp4}
      }

      \caption{Sample trials in (a) Exp.~1, (b) Exp.~2, (c) Exp.~3, and (d) Exp.~4.}
      \label{fig:trials}
      \end{figure}

      Each of the seven contents in \ref{stims} was combined with one of the seven items shown in \ref{items} in each of the four experiments.
     
      \ex.\label{items}
        \a. Jack saw the murder.
        \b. Raul cheated on his wife.
        \c. Ann stole the money.
        \d. Danny ate the last cupcake.
        \b. Lucy broke the plate.
        \b. Dustin lost his key.
        \b. Greg bought a new car.
        \z.

      Each experiment included two control stimuli serving as attention checks: one was expected to receive a response at one end of the slider (Exps.~1-3) or a `no' response (Exp.~4); the other control stimulus was expected to receive a response at the other end of the slider (Exps.~1-3) or a `yes' response (Exp.~4). See Supplement \ref{supp:stims} for the control stimuli used in Exps.~1-4.

      In all four experiments, each participant's set of items was generated by randomly combining each of the seven contents in \ref{stims} with a unique content in \ref{items}. Participants completed a total of 9 trials, namely 7 target trials and the same 2 control trials. Trial order was randomized.

      After completing the experiment, participants filled out a short optional demographic survey. To encourage truthful responses, participants were told that they would be paid no matter what answers they gave in the survey.

    \subsubsection{Data exclusion}

      We excluded the data of participants that were not self-reporteded native speakers of American English and of participants whose responses to one of the two control trials was more than 2 sd away from the group mean (Exps.~1--3) or whose responses to one of the two control trials was wrong (Exp.~4). Table \ref{t:excluded} shows how many participants were excluded in each experiment, demographic information for the remaining participants, and the number of data points that entered into the analyses.

      \begin{table}[h!]
        \centering
        \begin{tabular}{l | r r | r r  | r }
                     & \multicolumn{2}{c|}{\bf exclusion criterion} & \multicolumn{2}{c|}{\bf remaining participants} & data \\ 
                    & language & fillers & ages (mean age) & f/m/nb/dnd &  points \\ \hline
        Exp.~1 (QUD)   & 1 &  10 &  18-81 (41.1) & 36/32/0/1 & 621 \\ 
        Exp.~2 (asking whether) &  2 &  4 & 22-74 (38.7) & 45/27/1/1 & 666 \\ 
        Exp.~3  (direct dissent) &  2 &  7 & 18-77 (39.5) & 44/25/1/1  & 639 \\ 
        Exp.~4  (yes, but) & 4 & 4 & 19-67 (38.5)  & 43/27/2/0 & 648 \\ 
        \hline
        \end{tabular}
        \caption{Information from Exps.~1-4 about the number of participants whose data was excluded based on their self-declared language (variety) and the fillers, about the remaining participants, and about the number of data points that entered into the analysis.}\label{t:excluded}
      \end{table}

  \subsection{Results}
    Fig.~\ref{fig:results} plots the results of the four experiments by the expression that is associated with the seven target contents: panel (a) shows the mean naturalness ratings in Exp.~1 (QUD diagnostic), panel (b) mean `asking whether' ratings in Exp.~2 (`asking whether' diagnostic), panel (c) mean naturalness ratings in Exp.~3 (direct-dissent diagnostic) and panel (d) the proportion of `no' choices in Exp.~4 (`yes, but' diagnostic).

    \begin{figure}[h!]
        \centering
        % Top row
        \subfigure[Exp.~1 (QUD diagnostic)]{%
          \includegraphics[width=0.48\linewidth]{../../results+analysis/exps1-4/exp1/graphs/mean-ratings.pdf}%
          \label{fig:qud}
        }
        \hfill
        \subfigure[Exp.~2 (`asking whether' diagnostic)]{%
          \includegraphics[width=0.48\linewidth]{../../results+analysis/exps1-4/exp2/graphs/mean-ratings.pdf}%
          \label{fig:AK}
        }

        % \vspace{1em}

        % Bottom row
        \subfigure[Exp.~3 (direct-dissent diagnostic)]{%
          \includegraphics[width=0.48\textwidth]{../../results+analysis/exps1-4/exp3/graphs/mean-ratings.pdf}%
          \label{fig:dd}
        }
        \hfill
        \subfigure[Exp.~4 (`yes, but' diagnostic)]{%
          \includegraphics[width=0.48\textwidth]{../../results+analysis/exps1-4/exp4/graphs/mean-ratings.pdf}%
          \label{fig:yb}
        }
      \caption{Results of Exps.~1--4.
        Panels (a)--(c) show the mean responses by expression for (a) Exp.~1 (QUD diagnostic),  (b) Exp.~2 (`asking whether' diagnostic), and (c) Exp.~3 (direct-dissent diagnostic); panel (d) shows the proportion of `no' choices by expression in Exp.~4 (`yes, but' diagnostic). Error bars indicate 95\% bootstrapped confidence intervals. Violin plots in panels (a)-(c) show the kernel probability density of individual participants' ratings. Gray dots in panel (d) represent individual participant responses (`no' vs.\ `yes', jittered vertically and horizontally for legibility).}
      \label{fig:results}
      \end{figure}

    \subsubsection{Range of by-content means}
      We observe that the results of the four experiments differ in the range of the (mean or proportion of) ratings, that is, the difference between the largest and smallest means. The range is largest in Exp.~2 (`asking whether' diagnostic), at .74 (.1 to .83) and smallest in Exp.~3 (direct-dissent diagnostic), at .13 (.64 to .78). The results of Exp.~1 (QUD diagnostic, with a range of .27 (.51 to .77) and Exp.~4 (`yes, but' diagnostic), with a range of .46 (.5 to .96), fall in between. These results suggest that the four diagnostics, as implemented here, differ in how much they differentiate between the seven contents investigated, with the `asking whether' diagnostic showing the most differentiation and the direct-dissent and the QUD diagnostic showing the least.

      %Exp 1
        %min: 0.5055072
        %max: 0.7713043
        %range: 0.2657971

        %Exp 2
        %min: 0.09364865
        %max: 0.8332432
        %range: 0.7395946

        %Exp 3
        %min: 0.6443662
        %max: 0.7752113
        %range: 0.1308451

        %Exp 4
        %min: .5
        %max: 0.958
        %range: 0.458

    \subsubsection{Rank order and Spearman rank correlations} 
      We also find that the four experiments differ in the relative ratings assigned to the seven contents, with only limited overlap. Across all experiments, \emph{confirm} and \emph{discover} consistently received higher ratings (at least numerically) than \emph{confess}, which in turn received higher ratings than medial and final NRRCs. For all other pairs of expressions, however, the ordering was not consistent. In particular, the relative ranking of \emph{be right} and \emph{know} is inconsitent across experiments:  The embedded content of \emph{be right} received the lowest ratings in Exp.~1, but was among the most at-issue in the other three experiments. The embedded content of \emph{know} was rated (numerically) lower than that of \emph{confirm} in Exps.~1 and 2, but higher in Exps.~3 and 4.

      This difference between the results of the experiments is quantified in the Spearman rank correlations in Table \ref{t:spearman}.\footnote{The Spearman rank correlation coefficient, a value between -1 and 1, is a nonparametric measure of rank correlation: the higher the absolute value of the coefficient, the more the relation between the the two variables can be described using a monotonic function. If the coefficient is positive, the value of one variable tends to increase with an increase in the other. In the case of our experiments, a coefficient of 1 for two experiments would mean that there is a perfectly monotone increasing relation between the mean ratings of the seven contents in the two experiments: for any two contents c1 and c2, if c1 ranks below c2 in one experiment (that is, the mean rating of c1 is lower than that of c2), then that ranking is preserved in the other experiment.}

      \begin{table}[ht!]
        \centering
        \begin{tabular}{l | c c c c}
        & Exp.~1 & Exp.~2 & Exp.~3 & Exp.~4 \\ \hline
        Exp.~1 (QUD diagnostic) & \cellcolor{lightgray} & .11 & -.29 & -.18 \\
        Exp.~2 (`asking whether' diagnostic) & \cellcolor{lightgray} & \cellcolor{lightgray} & .64 &.79 \\
        Exp.~3 (direct-dissent diagnostic) & \cellcolor{lightgray}& \cellcolor{lightgray} & \cellcolor{lightgray} & .79  \\
        \hline
        % Exp.~4 (`yes, but' diagnostic) & & & & \cellcolor{lightgray} \\ \hline
        \end{tabular}
      \caption{Spearman rank correlations between the results of Exps.~1-4.}\label{t:spearman}
      \end{table}

      The rank correlations are particularly low for for Exp.~1 compared to the other three experiments, at least partly because of the low relative ranking of \emph{be right}. These results suggests that the four diagnostics as implemented in Exps.~1-4 interact differently with the seven contents investigated.

    \subsubsection{Pairwise differences between expressions}
      Finally, the experiments differ in whether they reproduce distinctions between contents reported in prior literature. Fig.~\ref{fig:pairwise} presents the results of posthoc pairwise comparisons of the estimated means/proportions for each content, using the `emmeans' package (\citealt{emmeans}) in R (\citealt{r}). The input to the pairwise comparisons were mixed-effects beta regression models (Exps.~1-3) or a mixed-effects logistic regression model (Exp.~4). All models were fit using the `brms' package (\citealt{buerkner2017}) using weakly informative priors. The models predicted the ratings\footnote{To model the ratings in Exps.~1-3 using a beta regression, the ratings were first transformed from the interval [0,1] to the interval (0,1) using the method proposed in \citealt{smithson-verkuilen2006}.} from a fixed effect of expression (with treatment coding and `be right' as reference level) and included random by-participant and by-item intercepts. The output of the pairwise comparison were 95\% highest density intervals (HDIs) of estimated marginal mean differences between each of the expressions. We assume that two contents differ if their HDI does not include 0.\footnote{The full model outputs are available in the folder \texttt{results+analysis/exps1-4/} in the repository linked in footnote \ref{f:github}.}

      Recall that \citealt{syrett_experimental_2015}, using a variant of the direct-dissent diagnostic, found that sentence-medial NRRCs are more not-at-issue than sentence-final ones. In contrast, as shown in Figs.~\ref{fig:results} and \ref{fig:pairwise}, no such difference is observed in Exps.~1-3; and in Exp.~4 (`yes, but' diagnostic), we find the opposite: sentence-final NRRCs are rated less at-issue than sentence-medial ones. Thus, none of the diagnostics as implemented in Exps.~1-4 replicate \citeauthor{syrett_experimental_2015}'s finding.

      Recall also that
      % \citealt{tonhauser_how_2018} and
      \citealt{degen-tonhauser-glossa}, using the `asking whether' diagnostic, observed  the following at-issueness differences among the content of the complement of clause-embedding predicates: \emph{know} < \emph{discover} < \emph{confess} < \emph{confirm} < \emph{be right} (where the embedded content of \emph{know} is least at-issue, and that of \emph{be right} is most at-issue). As shown in Figs.~\ref{fig:results} and \ref{fig:pairwise}, Exp.~2 (`asking whether') largely replicates this pattern, except that \emph{confess} and \emph{discover} do not differ.

      In Exp.~1 (QUD diagnostic), the embedded content of \emph{confirm} is more at-issue than that of \emph{confess, know,}, but the only other difference observed (among clause-embedding predicates) is that the embedded content of \emph{be right} is less at-issue than those of the other predicates --- the direction of this difference is the opposite from that observed in prior literature and the other experiments. In Exp.~3, no differences between the embedded contents of clause-embedding predicates are found. Finally, in Exp.~4, the content of the complement of \emph{be right} is more at-issue than those of \emph{confirm, confess,} and \emph{know}, that of \emph{confirm} is more at-issue than that of \emph{confess}, and that of \emph{discover} is more at-issue than that of \emph{confess}. These results suggest that the diagnostics, as implemented in Exps.~1-4, differ in whether they indicate differences in at-issueness between the contents of the complements of the five clause-embedding predicates included in the experiments.

      \addtolength{\tabcolsep}{-.19em}
      %
      \begin{figure}[!h]
        \centering
        \subfigure[Exp.~1 (QUD diagnostic)]{%
        \begin{tabular}{r | ccccccc}
        & \rots{be right} & \rots{confirm} & \rots{discover} & \rots{confess} & \rots{know} & \rots{final NRRC} & \rots{medial NRRC} \\
        \hline
        \input{../../results+analysis/exps1-4/exp1/models/table1}
        \hline
        \end{tabular}}
        \hfill
        \subfigure[Exp.~2 (`asking whether' diagnostic)]{%
        \begin{tabular}{r | ccccccc}
        & \rots{be right} & \rots{confirm} & \rots{discover} & \rots{confess} & \rots{know} & \rots{final NRRC} & \rots{medial NRRC} \\
        \hline
        \input{../../results+analysis/exps1-4/exp2/models/table1}
        \hline
        \end{tabular}}

        \subfigure[Exp.~3 (direct-dissent diagnostic)]{%
        \begin{tabular}{r | ccccccc}
        & \rots{be right} & \rots{confirm} & \rots{discover} & \rots{confess} & \rots{know} & \rots{final NRRC} & \rots{medial NRRC} \\
        \hline
        \input{../../results+analysis/exps1-4/exp3/models/table1}
        \hline
        \end{tabular}}
        \hfill
        \subfigure[Exp.~4 (`yes, but' diagnostic)]{%
        \begin{tabular}{r | ccccccc}
        & \rots{be right} & \rots{confirm} & \rots{discover} & \rots{confess} & \rots{know} & \rots{final NRRC} & \rots{medial NRRC} \\
        \hline
        \input{../../results+analysis/exps1-4/exp4/models/table1}
        \hline
        \end{tabular}}
      \caption{Pairwise differences between expressions,
        ordered from top to bottom and left to right by increasing mean in Exp.~2 (`asking whether' diagnostic). A white cell means that the 95\% HDI of the pair of the row expression and the column expression includes 0, a red cell means that the 95\% HDI does not include 0 and that the coefficient is positive (the row expression received a higher rating than the column expression), and a blue cell means that the 95\% HDI does not include 0 and the coefficient is negative (the row expression received a lower rating than the column expression).}
        \label{fig:pairwise}
      \end{figure}

      These results also lend further support to the above reported finding that the four diagnostics, as implemented here, differ in how much they differentiate between the seven contents investigated. In particular, while none of the experiments found differences between medial and final NRRCs, the results of Exp.~2 (`asking whether' diagnostic) distinguished between most of the contents of the complements of the five clause-embedding predicates, whereas the results of Exp.~3 (direct-dissent diagnostic) distinguished between the least of these contents. In line with the range of means/proportions reported above, the pattern of differences between contents suggests that the `asking whether' diagnostic showed the most differentiation, while the direct-dissent and QUD diagnostics showed the least.

    

  \subsection{Discussion}

    Exps.~1-4 were designed to compared the results of four different diagnostics of at-issueness that have been used in prior literature. The results of the experiments suggest that the diagnostics, in the particular way in which they were implemented in the experiments, differ on several dimensions.
    \begin{enumerate}
      \item They differ in the extent to which they differentiate between the seven contents investigated. with the `asking whether' diagnostic of Exp.~2 showing the most differentiation and the direct-dissent diagnostic of Exp.~3 showing the least, as evidenced by the range of means and amount of pairwise differences.

      \item They differ in the relative order of the seven contents. In particular, the relative ranking of \emph{be right} and \emph{know} differs across Exps.~1--4.

    \end{enumerate}

    This section offers discussion of these findings, particularly addressing the question why the `asking whether' exhibits the most differentiation among the diagnostics, why \emph{be right} in particular ranks high under all diagnstics except for the QUD diagnostic, and how the differences between diagnostics we found bear on whether they target distinct underlying notions of at-issueness.

    \subsubsection{Why does the `asking whether' diagnostic show greater differentiation?}
      One of the most noticable differences between the results of the experiments is that Exp.~2 (`asking whether') produced a much larger range of ratings and finer-grained differentiation among contents than the other diagnostics. Why might this be?
      %
      Prior work has emphasized a divide between question-based diagnostics (QUD, `asking whether') and assertion-based diagnostics (direct-dissent, `yes, but'): Question-based tests are taken to probe whether a proposition is at issue relative to a QUD introduced in discourse, wheras assertion-based tests are taken to target a different underlying notion (\citealt{snider_anaphoric_2017,snider_at-issueness_2017,snider_distinguishing_2018,koev_notions_2018,faller_discourse_2019,korotkova_evidential_2020}). This distinction, however, could not explain why the `asking whether' diagnostic exhibits a particular behavior to the exclusion of the QUD diagnostic, since both are question-based.

      There are two crucial differences that set apart the `asking whether' test from the other three: First, it is the only diagnostic where the target content itself occurs inside a polar question, whereas the other diagnostics test content embedded in declarative assertions. Second, it is the only diagnostic in which participants are asked directly about the intentions of the speaker, whereas the other ones collect measures that are more indirectly tied to semantic interpretation, such as acceptability ratings and continuation choices. Either of these distinctive characteristics could account for the greater differentiation. This yields two hypotheses, which we tested in Exps.~5--6, presented in the next section.

    \subsubsection{The QUD diagnostic and \emph{be right} \label{ssub:be-right}}
      Another striking difference between the results of the experiments concerns the content of the complement of \emph{be right} in Exp.~1 (QUD diagnostic) versus the other three experiments. As shown in panel (a) of Fig.~\ref{fig:results}, participants gave relatively low naturalness ratings to responses like that in \ref{beright.a}, suggesting that they did not view the response as fitting the question.
      % As discussed above, the content of the complement of \emph{be right} emerged as the least at-issue in Exp.~1. 

      \ex.
      \a.\label{beright.a}  Exp.~1 (QUD diagnostic) with \emph{be right}
      \\ {\bf Nora:} \emph{What did Lucy break?}
      \\ {\bf Leo:} \emph{Danny is right that she broke the plate.}
      \b.\label{beright.b} Exp.~2 (`asking whether' diagnostic )
      \\ {\bf Nora:} \emph{Is Danny right that she broke the plate?}
      \c.\label{beright.c} Exp.~3 (`direct dissent' diagnostic)
      \\ {\bf Nora:} \emph{Danny is right that she broke the plate.}
      \\ {\bf Leo:} \emph{No, she didn't break the plate.}
      \d.\label{beright.d} Exp.~4 (`yes, but' diagnostic)
      \\ {\bf Nora:} \emph{Danny is right that she broke the plate.}
      \\ {\bf Leo:} \emph{Yes, but she didn't break the plate.}
      \\ \hspace*{1cm} \emph{Yes, and she didn't break the plate.}
      \\ \hspace*{1cm} \emph{No, she didn't break the plate.}

      Why might this be? We hypothesize that this is because \emph{be right} in \Last presupposes that Danny has previously committed to the proposition that \enquote{Lucy broke the plate} (\citealt{abusch_presupposition_2010,anand_factivity_2014}). In the three diagnostics in \Last[b--d], no previous discourse context is given, so this presupposition can be accommodated.

      In contrast, in the QUD-diagnostic \Last[a], the preceding discourse context conflicts with the presupposition, making it difficult to accommodate. Specifically, we hypothesize that \emph{be right} signals that the question \enquote{whether Lucy broke the plate} is salient in the preceding discourse. This allows us to understand the ill-formedness based on QUD-based discourse structure, because this presupposed question is a subquestion (in the sense of \citealt{roberts_information_1996}) of the question in \Last[a] \enquote{What did Lucy break}, since every exhaustive answer to the latter entails an answer to the former. The progression from the presupposed subquestion question to the explicitly given superquestion violates Roberts' constraint on QUD stacks (p. 6:15, (10giii)), which allows only the reverse order (from superquestions to subquestions).

      As a result, the target utterance in \Last[a] is a relatively unsuitable answer not because the embedded clause fails to be at-issue,%
        \footnote{When \emph{be right} is excluded, the Spearman rank correlations are:

        \begin{tabular}{l | c c c c}
        & Exp.~1 & Exp.~2 & Exp.~3 & Exp.~4 \\ \hline
        Exp.~1 (QUD diagnostic) & \cellcolor{lightgray} & .77 & -.09 & -.31 \\
        Exp.~2 (`asking whether' diagnostic) & \cellcolor{lightgray} & \cellcolor{lightgray} & .66 & .66 \\
        Exp.~3 (`direct dissent' diagnostic) & \cellcolor{lightgray}& \cellcolor{lightgray} & \cellcolor{lightgray} & .77  \\
        \hline
        % Exp.~4 (`yes, but' diagnostic) & & & & \cellcolor{lightgray} \\ \hline
        \end{tabular}}
      but because it presupposes an incoherent discourse context. Therefore, when using a particular diagnostic, one must also consider whether the expressions used introduce presuppositional requirements on the discourse that may be independent of whether the target content is interpreted as at-issue.


    \subsubsection{Medial vs. final appositive NRRCs}
        As mentioned above, none of our experiments replicated the effect reported in \citealt{syrett_experimental_2015}, that sentence-final NRRCs were more at-issue than sentence-medial ones. This includes Exp.~3, which implemented a variant of the `direct dissent' diagnostic that was used in \citealt{syrett_experimental_2015}. A difference between our Exp.~3 and Exp.~2 in \citealt{syrett_experimental_2015}, aside from the stimuli, is that we asked for naturalness ratings and they implemented it as a forced choice: do you choose to dissent with the main clause content or the content of the NRRC. An interesting follow-up would be to see whether our stimuli with the forced choice can reproduce their result. However, our Exp.~4 in which we implemented an assent/dissent-based forced-choice task shows the opposite effect: Here, medial NRRCs were more at-issue than final ones.

        These different findings suggest that even small changes in the response task might matter and also that the diagnostics differ in whether they detect differences in at-issueness between sentence-medial and -final NRRCs.


    \subsubsection{Interim summary}
      The findings from Exps.~1--4 show that the four diagnostics are not interchangeable: they differ in how strongly they differentiate among the tested contents, in the relative ranking between them, and in how they interact with discourse requirements imposed by particular expressions (as illustrated by the \emph{be right} case under the QUD diagnostic).

      Most strikingly, the `asking whether' diagnostic stands out for producing greater differentiation across contents than the others. Two factors emerge as plausible explanations for this pattern: (i) the target content appears inside a question rather than an assertion, and (ii) the response task directly probes what the utterance is about, rather than relying on indirect measures such as naturalness judgments or choosing the most natural response.

      The next section reports on Exps.~ 5 and 6, which compare the `asking whether' test to diagnostic that also embeds the target content in a question but uses naturalness ratings for a direct response to assess at-issueness. Since both experiments yielded similarly fine-grained distinctions among contents, we take this as evidence that the greater variation observed with asking whether' stems from question embedding rather than the response task.

        
\section{Experiments 5 and 6 \label{sec:3_more-experiments}}
  %
    Exps.~5 and 6 investigate whether the greater differentiation observed in Exp.~2 (`asking whether') was due to (i) embedding target contents in polar questions or (ii) the response task, testing two hypotheses:
    \begin{enumerate}
      \item \textbf{Question-embedding hypothesis:} Differentiation between contents (in terms of range of means and significant differences) is greater when contents are embedded in a polar question than in a declarative assertion.

      \item \textbf{Response-task hypothesis:} Differentiation is greater when participants are asked directly what the utterance is about, compared to tasks such as acceptability judgments or forced-choice continuations.

    \end{enumerate}

    To test these hypotheses, we compared two diagnostics that both embed the target content in a polar question but differ in the response task. Exp.~5 used the `asking whether' diagnostic as in Exp.~2 \Next[a] and Exp.~6 uses a `direct response' diagnostic, shown in \Next[b], where participants read a dialogue between two named speakers, where the first utters a polar question, which contributed the target content. Like in the direct-assent/dissent diagnostic, the second speaker utters `yes' answer and affirms the target content.

    \ex. Implementation of the diagnostics in Exps.~  5--6
      \a.\label{exp5} Exp.~5 (`asking whether' diagnostic )
      \\ {\bf Nora:} \emph{Is Tom right that Lucy broke the plate?}
      \\ Question to participants: Is Nora asking whether Lucy broke the plate?
      \b.\label{exp6} Exp.~6 (direct-response diagnostic)
      \\ {\bf Nora:} \emph{Is Tom right that Lucy broke the plate?}
      \\ {\bf Leo:} \emph{Yes, she didn't break the plate.}
      \\ Question to participants: How natural is Leo's response to Nora's question?


    Both experiments measured at-issueness for the contents of the complements of the 20 clause-embedding predicates from \citealt{tonhauser_how_2018} and \citealt{degen-tonhauser-glossa}, listed in \Next.

    \ex. \label{ex:predicates}
        20 clause-embedding predicates from \citealt{tonhauser_how_2018,degen-tonhauser-glossa}:\\ 
        \emph{be annoyed, know, pretend, inform, see, hear, discover, acknowledge, think, admit, announce, reveal, confess, demonstrate, suggest, prove, establish, say, confirm, be right}
        \z. 
      
    Both experiments also measured projection data for the embedded contents of the 20 clause-embedding predicates, which were reported in \citealt{hofmann-etal2024}. Here we focus on the at-issueness data, which have not yet been reported.

    If the greater differentiation between contents found in Exp.~2 (compared to Exps.~1, 3,  and 4) was due to presenting the target content in a polar question, we expect that results from Exps.~5 and 6 both show a similar differentiation between contents---namely comparable range of means and comparable sensitivity to differences between contents. In contrast, if the `asking whether' diagnostic again shows greater differentiatiation between contents than the direct-response diagnostic, this would support the response-task hypothesis, suggesting that the task, rather the question embedding, drives the effect.

  \subsection{Methods}
    
    \subsubsection{Participants}

      We recruited 300 participants on Amazon Mechanical Turk for Exp.~5 and 250 participants on Prolific for Exp.~6.\footnote{Exp.~5 was run in August 2019 and Exp.~6 in August 2021.} Participants recruited on Mechanical Turk had U.S.\ IP addresses and at least 99\% of previous HITs approved. Participants recruited on Prolific had registered as U.S.-born native speakers of English residing in the USA and had an approval rate of at least 99\%. Table~\ref{t:recruited2} summarizes the age and gender distributions of the recruited participants.

      \begin{table}[h!]
      \centering
      \begin{tabular}{l | c | r r r }
                  & recruited & ages (mean age) & f/m/nb/dnd \\ \hline
      Exp.~5 (asking whether) & 300 & 19-74 (38.2) & --/--/--/--  \\
      Exp.~6 (direct assent) & 250 & 18-58 (25.5)  & 201/43/6/0  \\
      \hline
      \end{tabular}

      \caption{Information about the participants recruited in Exps.~5-6 (f = female, m = male, nb = nonbinary, dnd = did not disclose; gender information was not collected in Exp.~5).}\label{t:recruited2}
      \end{table}

    \subsubsection{Materials and procedure}

      Each of the 20 clause-embedding predicates in \ref{ex:predicates}, above, was combined with one of 20 embedded-clause items (listed in Supplement \ref{supp:a-clauses}). The two experiments also included the same six control stimuli. The contents of these control stimuli (details in Supplement \ref{supp:control56}) were expected to be at-issue and were used to assess participants' attention. In both experiments, each participant's set of items was generated by randomly combining each of the 20 clause-embedding predicates in \ref{ex:predicates} with a unique item. Participants saw a total of 26 stimuli: namely one target stimulus for each of the 20 clause-embedding predicates and the same 6 control trials.\footnote{Each participant saw their set of 26 stimuli twice, once in the projection block and once in the at-issueness block. Block order was randomized. As mentioned above, we focus here on the at-issueness ratings.} Trial order was randomized.
  	
      Participants were asked to imagine that they are at a party and that, when walking into the kitchen, they overhear somebody say something to somebody else.

      As shown in Fig.~\ref{fig:trials56}, the response task differed between experiments. In Exp.~5 (`asking whether' diagnostic, panel (a)), participants judged whether the question was about the target content, using a slider marked `no' on one end (coded as 0) and `yes' on the other (coded as 1). In Exp.~6 (direct-response diagnostic, panel (b)), participants rated whether the response to the first speaker sounds good, on a slider marked `no' (coded as 0) on one end and `yes' on the other (coded as 1). Across both experiments, the responses were coded so that 1 meant that the content to be diagnosed was rated as at-issue and 0 as not-at-issue.

      \begin{figure}[h!]
      \centering
      % Top row
      \subfigure[Exp.~5: `asking whether' diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp5}}%
        \label{fig:trialExp5}
      }
      \hfill
      \subfigure[Exp.~6: direct response diagnostic]{%
        \fbox{\includegraphics[width=0.47\textwidth]{figures/trialExp6}}%
        \label{fig:trialExp6}
      }

      \caption{Sample trials in (a) Exp.~5 and (b) Exp.~6.}
      \label{fig:trials56}
      \end{figure}

      After completing the experiment, participants filled out a short optional demographic survey. To encourage truthful responses, participants were told that they would be paid no matter what answers they gave in the survey.

    \subsubsection{Data exclusion}
    
      We excluded the data of participants who did not self-identify as native speakers of American English, of participants whose responses to the projection or at-issueness controls were more than 2 sd away from the group mean, and of participants who always selected roughly the same point on the response scale for the target stimuli. To identify such participants, we first identified participants whose mean variance on the target stimuli was more than 2 sd below the group mean variance and then manually inspecting their response patterns. Due to a programming error, 5 participants took Exp.~5 more than once. Since we were not able to identify which submission was their first submission, the data of these participants was also excluded. Table \ref{t:excluded2} shows how many participants were excluded in each experiment, the properties of the remaining participants, and the number of data points that entered into the analyses.

      % {\bf table should show final number of participants, exp 5: 242; exp 6: }
        
      \begin{table}[h!]
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{l | r r r | r r r | r }
                     & \multicolumn{3}{c|}{\bf exclusion criterion} & \multicolumn{3}{c|}{\bf remaining participants} & data \\ 
                    & language & controls & variance & ages (mean age) & f/m/nb/dnd & total &  points \\ \hline
        Exp.~5 (asking whether)  & 7 &  35 & 0 &  21-74 (39.2) & --/--/--/-- & 242 & 6292 \\ 
        Exp.~6 (direct assent) &  5 &  24 & 1 & 18-58 (24.9) & 187/28/5/0 & 220 & 5720 \\ 
        \hline
        \end{tabular}}
        \caption{Information from Exps.~5-6 about the number of participants whose data was excluded based on their self-declared language and language variety (`language'), the controls, and the variance of their responses, about the remaining participants, and about the number of at-issueness data points that entered into the analysis.}\label{t:excluded2}
        \end{table}

  \subsection{Results}
    Fig.~\ref{fig:results2} plots the results of the two experiments by embedding predicate: panel (a) shows the mean `asking whether' ratings in Exp.~5 (`asking whether' diagnostic) and panel (b) shows the mean acceptability ratings in Exp.~6 (direct-response diagnostic).
  


    \begin{figure}[h!]
      \centering
      
      \subfigure[Exp.~5 (`asking whether' diagnostic)]{%
        \includegraphics[width=0.7\linewidth]{../../results+analysis/exps5-6/exp5/graphs/mean-ratings.pdf}%
        \label{fig:results5}
      }

      \subfigure[Exp.~6 (`direct assent' diagnostic)]{%
        \includegraphics[width=0.7\linewidth]{../../results+analysis/exps5-6/exp6/graphs/mean-ratings.pdf}%
        \label{fig:results6}
      }

      \caption{Results of Exps.~5--6. The panels show the mean ratings by expression for (a) Exp.~5 (asking whether diagnostic) and (b) Exp.~2 (direct assent diagnostic). Error bars indicate 95\% bootstrapped confidence intervals. Violin plots show the kernel probability density of individual participants' ratings.}
      \label{fig:results2}
    \end{figure}


    \subsubsection{Range of by-content means} 

      We observe that the results of the four experiments show a similar range of the mean ratings (again quantified as the difference between the largest and smallest by-content means). The range in Exp.~5 (`asking whether' diagnostic) is .75 (.07 to .82) and in Exp.~6 (direct-response diagnostic), it is .73 (.09 to .82). While results of Exps.~1--4 showed clear variation in the range of the by-content means, the same cannot be said about the results of Exps.~5 and 6.

    \subsubsection{Rank order and Spearman rank correlations} 
        
      We also find a high degree of consistency in the relative ratings across the 20 contents: predicates whose embedded content is rated relatively at-issue in Exp.~5 are also rated relatively at-issue in Exp.~6, and vice-versa.This correspondence is reflected in a Spearman rank correlation of $.93$ between the results of Exps.~5 and 6, suggesting a particularly strong rank correlation for the by-content means. This result suggests that the two diagnostics as implemented in Exps.~5 and 6 interact similarly with the 20 target expressions investigated, when those are presented in a question embedding.

    \subsubsection{Pairwise differences between expressions}  
      
      Fig.~\ref{fig:pairwise2} presents the results of post-hoc pairwise comparisons of the estimated means for each content. As in \Cref{sec:2_experiments}, these were done using the `emmeans' package (\citealt{emmeans}) in R (\citealt{r}), based on mixed-effects beta regression models, that were fit using the `brms' package (\citealt{buerkner2017}) using weakly informative priors.  The models predicted ratings from a fixed effect of expression (with treatment coding and `be right' as the reference level).

      \begin{figure}[!h]
      \centering

      \subfigure[Exp.~5 (`asking whether' diagnostic)]{%
      \resizebox{.65\linewidth}{!}{
      \begin{tabular}{r | ccccc ccccc ccccc ccccc}
      & \rots{be right} & \rots{confirm} & \rots{say} & \rots{establish} & \rots{prove} & \rots{suggest} & \rots{demonstrate} & \rots{announce} & \rots{reveal} & \rots{admit} & \rots{confess} & \rots{think} & \rots{acknowledge} & \rots{pretend} & \rots{see} & \rots{discover} & \rots{hear} & \rots{inform} & \rots{know} & \rots{be annoyed} \\
      \hline
      \input{../../results+analysis/exps5-6/exp5/models/table1}
      \hline
      \end{tabular}}}

      \subfigure[Exp.~6 (`direct assent' diagnostic)]{%
      \resizebox{.65\linewidth}{!}{
      \begin{tabular}{r | ccccc ccccc ccccc ccccc}
      & \rots{be right} & \rots{confirm} & \rots{say} & \rots{establish} & \rots{prove} & \rots{suggest} & \rots{demonstrate} & \rots{announce} & \rots{reveal} & \rots{admit} & \rots{confess} & \rots{think} & \rots{acknowledge} & \rots{pretend} & \rots{see} & \rots{discover} & \rots{hear} & \rots{inform} & \rots{know} & \rots{be annoyed} \\
      \hline
      \input{../../results+analysis/exps5-6/exp6/models/table1}
      \hline
      \end{tabular}}}

      \caption{Pairwise differences between expressions, ordered from by increasing mean in Exp.~5 (`asking whether'). White cells indicate that the 95\% HDI of the difference includes 0. Red cells indicate a positive difference (the row expression received a higher rating than the column expression), and blue cells indicate a negative difference.}\label{fig:pairwise2}
      \end{figure}

      Overall, the two experiments show broadly similar results: both reveal significant pairwise distinctions among many of the expressions and yield comparable rank orderings. The ordering among the clause-embedding predicates found in \citealt{degen-tonhauser-glossa}\emph{know} < \emph{discover} < \emph{confess} < \emph{confirm} < \emph{be right} (which were largely replicated in Exp.~2) is found in both Exps.~5 and~6.

      There are some differences between the two experiments: In Exp.~5, the complement predicate \emph{demonstrate} is more at issue than the complement of a range of predicates, including \emph{announce, reveal,} and \emph{confess}, but in Exp.~6, the embedded content of \emph{demonstrate} is comes out as less at issue than the content of those three predicates.

        Other differences between the two experiments include that in Exp.~5 \emph{prove} and \emph{suggest} are more at-issue than \emph{reveal} and \emph{confess}, \emph{think} and \emph{acknowledge} are more at-issue than \emph{discover} and \emph{hear}, \emph{pretend} and \emph{see} are more at issue than \emph{inform} and \emph{know}, where Exp.~6 finds no differences. Conversely, in Exp.~6 \emph{confirm} and \emph{say} are more at-issue than \emph{establish}, \emph{pretend} is less at issue than \emph{discover} and \emph{hear}, \emph{see} is less at issue than \emph{discover}, where Exp.~5 finds no difference.

      There are, however, some minor differences between the two experiments. In Exp.~5, the complement of \emph{demonstrate} is rated as more at-issue than those of \emph{announce}, \emph{reveal}, and \emph{confess}, whereas in Exp.~6 it is rated as less at-issue than the complement of those same predicates.

      In the results of Exp.~5, several differences also come out as reliable, where no difference is observed in Exp.~6: (i) \emph{prove} and \emph{suggest} are more at-issue than \emph{reveal}; (ii) \emph{confess}, \emph{think}, and \emph{acknowledge} are more at-issue than \emph{discover} and \emph{hear}; and (iii) \emph{pretend} and \emph{see} are more at-issue than \emph{inform} and \emph{know}. Conversely, Exp.~6 reveals differences not found in Exp.~5, including that: (i) \emph{confirm} and \emph{say} are more at-issue than \emph{establish}; (ii) \emph{pretend} is less at-issue than \emph{discover} and \emph{hear}; and (iii) \emph{see} is less at-issue than \emph{discover}.

      Taken together, these results suggest that while the diagnostics as implemented in Exps.~5 and 6, may differ slightly in which distinctions they detect, their overall patterns of at-issueness judgments are highly similar.

  % \addtolength{\tabcolsep}{-.19em}

    

  \subsection{Discussion}

    Our results suggest that the `asking whether' and direct response diagnostic, as implemented in Exps.~5 and 6 do not differ greatly in how much they differentiate between the 20 contents investigated: Both show a similar range of by-content means, a particularly strong Spearman rank correlation, and fine-grained distinctions between the contents tested. The close similarity between the two experiments indicates that the high by-content differentiation observed in both of them is likely driven by their shared feature of presenting the target content embedded in a question. 

    Nonetheless, small differences remain, for example in which by-content contrasts reach significance and in the precise rank ordering of contents. Since the two experiments use the same embedding context for the target contents and both reflect a QUD-based conception of at-issueness, these differences may stem from the distinct response tasks used. This suggests that differences in response task, such as judging what a question is about versus evaluating the naturalness of a response, can influence measures of at-issueness.


\section{General discussion \label{sec:4_discussion}}
  %
    Different diagnostics of at-issueness yield different results. Some of the differences between the diagnostics as implemented here appear to be due to presenting the target expressions embedded in questions, some of them have to do with how the diagnostics interact differently with the discourse requirements imposed by the tested expressions, and others may have to do with response task differences, or different underlying notions of at-issueness.

    In our investigation, the most central factor turned out to be whether or not the tested target content is embedded in a question, so the speech act in which it appears needs to be taken into consideration when considering what at-issueness is and how it is diagnosed.

    The diagnostic differences also have methodological implications: future research will need to make sure that results may only be due to the speech act, particular response task used, other discourse factors, empirical investigations should continue path of using multiple diagnostics 

    In the following, we discuss the questions of why question embeddings matter for diagnosing at-issueness (\Cref{ssec:discussion-questions}); what are factors that may affect the diagnostic differences, and what this can tell us about different underlying notions of at-issueness (\Cref{ssec:discussion-differences}); and what are the methodological implications of the diagnostic differences we found (\Cref{ssec:discussion-methodology}).

    Summarizing findings and discussion from Exps.~1--4 in \Cref{sec:2_experiments}:
      \begin{itemize}

        \item \textbf{`asking whether':} The `asking whether' diagnostic (Exp.~2) showed the greatest differentiation between contents, whereas the other three experiments exhibited a smaller range of means, and fewer statistically reliable differences between investigated contents. Among the first 4 experiments, Exp.~2 was the only one that came close to differentiating fine-grained by-predicate differences as reported in \cite{degen-tonhauser-glossa}. This suggests that something that is present in the `asking whether' test, but not the other three allows for greater by-content differentiation. Embedding the target content in a question, or the response task, asking participants to judge speaker intentions were what we identified as likely explanatory factors.

        \item \textbf{Positional effects for appositive NRRCs?} \citepos{syrett_experimental_2015} found that sentence-final appositive NRRCs are more at-issue than sentence-medial ones under a version of the direct-dissent diagnostic. We did not reproduce this finding with any of the diagnostics. in fact, the assent/dissent-based `yes but' test showed the opposite effect: In our Exp.~4, sentence medial appositive NRRCs were more at-issue than sentence-final ones. 

        \item \textbf{\emph{be right} under the QUD diagnostic:} receives low ratings not because it's not at-issue, but because how the diagnostic interacts with a contextual requirement of \emph{be right} (namely the presupposition that there was a QUD about the embedded clause in the previous discourse, see discussion in \Cref{ssub:be-right}).

      \end{itemize}


    Summarizing findings and discussion from Exps.~5 and 6 in \Cref{sec:3_more-experiments}:

      \begin{itemize}
        \item \textbf{Similarity between diagnostics:} The `asking whether' and direct-response diagnostics yielded highly similar results: both showed comparable ranges of by-content means, a strong Spearman rank correlation ($r_s = .93$), and fine-grained differentiation among the 20 contents. This suggests that their shared property of question embedding drives the greater differentiation observed in Exps.~2, 5, and 6 compared to Exps.~1, 3, and 4.

        \item \textbf{Minor differences and task effects:} Despite their overall similarity, the two diagnostics diverged slightly in which by-content contrasts reached significance and in their rank orderings. Since both tests used identical embedding contexts, these small differences likely stem from the response taskjudging what a question is about versus evaluating the naturalness of a responseindicating that even subtle differences in task design can affect at-issueness judgments.

      \end{itemize}
  
  \subsection{Speech act, at-issueness, and projection \label{ssec:discussion-questions}}
    We found that the speech act in which the tested content appears makes a clear difference: presenting the target expression in a question leads to greater differentiation among contents than presenting it in an assertion. At first glance, this contrast might seem to reflect the contrast highlighted in previous literature between question-based and assertion-based diagnostics for at-issueness.  

    Recall that question-based diagnostics (such as the QUD and `asking whether' tests) have been taken to assess whether a proposition is at issue relative to a question under discussion (\citealt{amaral_review_2007,simons_what_2010,tonhauser_diagnosing_2012,tonhauser_how_2018}).
    %
    % The QUD diagnostic, illustrated in \ref{qud}, repeated here, tests whether some propositional content ($p:$ \emph{Greg bought a new car}) introduced by a particular type of expression (e.g., sentence-medial appositive NRRCs) can be felicitously used to answer an explicit QUD. The `asking whether' diagnostic, illustrated in \ref{aw}, also repeated below, tests whether the content $p$ introduced by some expression can contribute to determining what the QUD is.
    %
    % \ex.[\ref{qud}]%
    %   % QUD diagnostic (e.g., \citealt{tonhauser_diagnosing_2012,chen_presuppositions_2024})
    %   \a.[A:] \emph{What did Greg buy?}
    %   \b.[B:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    %   \z.
    %   Question to participants: How well does B's response fit A's question?
    % \z.
    %
    % \ex.[\ref{aw}]%
    %   % `asking whether' diagnostic (e.g., \citealt{tonhauser_how_2018,solstad_cataphoric_2024})\smallskip\\
    %     \emph{Is Greg, who bought a new car, envied by his neighbor?}\smallskip
    % \\ Question to participants: Is the speaker asking whether Greg bought a new car?
    % \z.
    %
    In contrast, assertion-based diagnostics (such as the direct-dissent and `yes, but' tests) have been suggested to target a different notion of at-issueness, tied to the speech act of assertion (\citealt{koev_notions_2018,faller_discourse_2019,korotkova_evidential_2020}). The assumption is that only at-issue content can contribute an assertive proposal that may be accepted or rejected, while not-at-issue content is presupposed or automatically added to the common ground (\citealt{potts_logic_2005,murray_varieties_2014,anderbois_at-issue_2015}).
    %
    % In the direct-dissent diagnostic illustrated in \ref{dd}, repeated here, it is tested whether the target content $p$ introduced by some expression can be felicitously targeted by a dissenting direct response that includes \emph{no}. The `yes but' test, \ref{yesbut}, repeated below, tests whether the proposal can be accepted with a direct response using \emph{yes}, while challenging the not-at-issue content, which is assumed to be independent of the at-issue proposal.
    %
    % \ex.[\ref{dd}]
    %   % Direct-dissent diagnostic (e.g., \citealt{tonhauser_diagnosing_2012,syrett_experimental_2015})
    %   \a.[A:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    %   \b.[B:]\emph{No, that's not true, he didn't buy a new car.}
    %   \z.
    % Question to participants: How natural is B's rejection of A's utterance?
    % \z.
    %
    % \ex.[\ref{yesbut}]%
    %   % `yes, but' diagnostic (e.g., \citealt{xue_correlation_2011,destruel_cross-linguistic_2015})
    %   \a.[A:] \emph{Greg, who bought a new car, is envied by his neighbor.}
    %   \b.[B:] \emph{Yes, but he didn't buy a new car.} /
    %   \b.[] \emph{Yes, and he didn't buy a new car.} /
    %   \b.[] \emph{No, he didn't buy a new car.}
    %   \z.
    %   Task for participants: Choose the response that sounds best.
    % \z.


    Our generalization, however, is not about whether the target content can be understood as at-issue relative to a QUD or an assertive proposal. Rather, it concerns the speech act in which the content is presented. Although both the `asking whether' and QUD diagnostics are based on QUD-based conceptions of at-issueness, they differ in this crucial respect: only the former presents the target content inside an interrogative, while the latter embeds it in a declarative.

    This distinction helps explain our findings. The high differentiation observed with the `asking whether' diagnostic (Exps.~2, 5) as well as the direct-response diagnostic used in Exp.~6 appears to arise from the question embedding itself. When the same contents are presented in a declarative assertion, as in the QUD diagnostic, the resulting ratings show less differentiation. This suggests that what matters for the differentiation is not the distinction between QUD-at-issueness and assertion-based notions, but the speech act in which the content is interpreted.

    We hypothesize that the greater differentiation observed when the target content is presented in a question arises from how speech act type and not-at-issue content both interact with speaker commitments. The argument goes as follows: (i) When propositional content is not-at-issue, it projects and and we understand the speaker to be committed to its truth. (ii) In assertions, but not in questions, the speaker is also committed to the at-issue content. (iii) As a result, it is easier for participants to distinguish at-issue from not-at-issue content in questions, when the speaker is not committed to the at-issue proposition, because the two contents are pragmatically more distinct.

    

    To consider this suggestion in more detail, we will first illustrate for questions, that at-issue and not-at-issue content has distinct epistemic status relative to speaker commitments: 
    %
    In questions, the at-issue content introduces question alternatives and partitions the context set without committing the speaker to any particular alternative (\citealt{groenendijk_studies_1984,ginzburg_interrogatives_1996,roberts_information_1996}). For the kind of polar question used in Exp.~2 (`asking whether', see \ref{aw}), the alternatives are the at-issue content and its opposite, while the not-at-issue content is assumed to be true in either alternative (\citealt{abusch_presupposition_2010,abrusan_predicting_2011,tonhauser_how_2018}):

    \ex. $\llbracket\textit{Is Greg, who bought a car, envied by his neighbor?}\rrbracket =$\\
      \phantom. \hfill$\{\textnormal{Greg, who bought a car, is envied by his neighbor},$\\
      \phantom. \hfill$\textnormal{Greg, who bought a car, is not envied by his neighbor}\}$
    \z.

    Here, the not-at-issue proposition (\emph{Greg bought a car}) is assumed to be part of the speaker commitments, that is, it projects, whereas the matrix at-issue content imposes a partition (see \citealt{potts_logic_2005,abusch_presupposition_2010,tonhauser_how_2018,degen-tonhauser-glossa}). We suggest that the lack of speaker commitment to the at-issue content makes it easier for listeners to distinguish between the two levels of meaning. Questions, therefore, are a particularly a good context to test fine-grained at-issueness differences that are associated with the overt forms or literal meanings.

    The assumption that overt forms or literal meanings can be associated with different at issueness 

    for instance, the literal meaning of appositives

    or the lexical semantics of clause-embedding verbs affects how strongly or how likely the embedded content is interpreted as at issue.


    \textbf{Lexical and compositional effects:} How strongly an embedded clause is interpreted as at-issue depends on the lexical semantics of the embedding predicate and the literal meaning of appositives or other constructions.

    Certain predicates more readily contribute their complements as not-at-issue content, 

    for some content $p$ introduced by some expression, such as the proposition that \emph{Tony had a drink last night} introduced by the complement of \emph{discover} in \Next.

    \ex. \emph{Did Helen confirm that Tony had a drink last night?} \z. 
     
    how much participants take the question to be about \emph{whether} $p$, how naturally a direct response such as \emph{yes} $p$ or \emph{no, not} $p$ is taken as an answer to the question, i.e., as differentiating between the question alternatives.

    then, we can say that the complement of \emph{confirm} is more (likely) to be at issue than the content of NRRCs

    Big question here: which lexical properties can affect this and how? Look to literature on presupposition triggering? (\citealt{abrusan_predicting_2011,schlenker_triggering_2021,anand_facts_2024,scontras_projection_2025})



    While the question meaning in \Last can be expressed without committing the speaker to any particular alternative, declarative assertions usually involve speaker commitment that the at-issue content is true. \textbf{Assertions:} Signal that speaker is committed to the at-issue content, and therefore at-issue content and not-at-issue content are harder to distinguish 
    Testing at-issueness of contents in declarative assertions muddles the picture, making at-issue and not-at-issue content harder to distinguish; especially when the test itself invokes commitment (like those using assent/dissent)

    % \ex. $\llbracket$\emph{Is Greg, who bought a car, envied by his neighbor?}$\rrbracket = \{$Greg, who bought a car, is envied by his neighbor$,$\\ \hspace*{7.2em}$Greg, who bought a car, is not envied by his neighbor$\}$  

    \textbf{Assertions:} In declarative assertions, by contrast, the speaker commits to the at-issue proposition itself, and not-at-issue content projects alongside it as a pragmatic entailment. Because both kinds of content end up as parts of the speakers commitments, they are harder to tease apart experimentally. Diagnostics that rely on assent or dissent (e.g., yes, no, yes, but) may thus blur distinctions between the main assertion and its associated presuppositions or entailments.  
    \ex. \a. Cole didnt admit that Julian dances salsa.\\
         \b. Yes, thats trueJulian dances salsa.\\


    This example illustrates how the logical relation between the matrix clause and its complement complicates assent/dissent judgments: saying yes may express agreement with the matrix, the embedded content, or both.  


    \textbf{Theoretical implication:} Because questions suspend speaker commitment, they more transparently reveal how different contents can function as at-issue or not-at-issue. Assertions, by contrast, conflate commitment with projection, obscuring these distinctions. Consequently, question-based diagnosticsespecially those embedding the target content directly in a polar questionprovide a particularly sensitive test of at-issueness.  

    the most sensitive test for at-issueness involve question embeddings


    
    \begin{itemize}
        \item therefore, it is not clear what it means to agree with an assertive utterance as a whole by saying \emph{yes/no} to that utterance, when we are agreeing/disagreeing with the non-at-issue content.

        \item when we say yes, we might want to say yes to the main assertion, and to some extent its other inferences, and the logical relationship between them plays a role as well

        \item recall the direct assent test with negated assertions (I should go back to my notes there, to make this point about the confound with assertion and commitment clearer again)

        \ex. \a. Cole didn't admit that Julian dances salsa.
          \b. Yes, that's true, Julian dances salsa.
          \z.
        \z. 

        \item it is important, what's the logical relationship between the main clause content, and embedded content (entailment, non-entailment, negation)

    \end{itemize}

    \begin{itemize}
        
        \item however, the QUD-test still tests the at-issueness of content in an assertive utterance relative to a preceding question. no previous literature has suggested that the speech act of the content being tested itself plays a large role.
        \item Polar questions highlight differences in at-issueness by showing how well a proposition can partition the context set: some contents do this clearly, others less so, and some only partially. By contrast, the other diagnostics depend on anaphoric accessibility (e.g., suitability for response particles) or on how well an assertion addresses a prior QUD, which may blur such distinctions.

        \item We hypothesize that the polar question embedding highlights at-issueness differences. Content is differently suitable to partition the context set, with some content really well able to do this and other content not, and other content inbetween. All the other diagnostics rely on anaphoric accessibility of the proposition or how well it can address a question in prior discourse.

        \item To our knowledge, no previous literature has proposed that the speech act of the tested content itself could be a major driver of differentiation.

      \end{itemize}




  \subsection{Diagnostic differences and notions of at-issueness \label{ssec:discussion-differences}}
    First, we aimed to determine the extent of consistency or divergence across these diagnostics when applied to identical linguistic stimuli. Different diagnostics give different results.

    results from Exps.~1-4 suggesting that the four diagnostics implemented there interact differently with the seven contents investigated: no pairwise difference goes the same way in all experiments except for \emph{confess} $\leq$ \emph{discover}

    the diverging results suggest that the diagnostics cannot be applied  interchangeably, and theoretical claims made on their basis should be relativized to the diagnostic used (see also \citealt{snider_anaphoric_2017,snider_at-issueness_2017,snider_distinguishing_2018,koev_notions_2018,korotkova_evidential_2020})


    Our findings suggest that differences between the diagnostics are affected by the speech act in which the target contents are presented, different way in which they interact with contextual requirements of the target expressions (be right, anaphoric availability)
    - response task (evident in the small differences between Exps.~5 and 6, as well the lack of replication of Syrett and Koev's positional effect on the at-issueness of appositive NRRCs).

    The influence of the speech act in which the target is presented was evident in ...

    The reason for believing that 

    Another reason for believing that the diagnostics may differ wrt the contextual requirements imposed is the point made by Snider, who noted that the direct-dissent diagnostic, and `yes but' diagnostic involve propositional anaphora in the interpretation in the polar response particles \emph{yes/no} (the same applies to the direct-response diagnostic in Exp.~6). While our data does not provide evidence that all diagnostics involving response particles (Exps.~3, 4, and 6) behave differently from the others


     \subsubsection{Medial and final appositives: no replication of S+K}
        \begin{itemize}
          \item 
          \item no evidence that there is a notion of at-issueness under which final appositives are more at-issue than medial ones.
        \end{itemize}

      \begin{itemize}
        \item range + sensitvity: differences in how the diagnostics differentiate between contents; why is this? ceiling effects? compare to NAI control means // there is a question here of whether there would be a significant difference between the AI-control, and the other contents. someone who believes in binary at-issueness could say that all it needs for the test to work is that the tested contents lead to less acceptability compared to AI control.
      \end{itemize}

    \subsubsection{Correspondences/similarities}

      \begin{itemize}
        \item there is a correlation of the results of the `asking whether' diagnostic with those of direct dissent and `yes, but' diagnostics
        \item when excluding the \emph{be right} condition, there is a correlation of QUD one with the other ones, too (?) --- these are the correlations we should be looking at!
        \item now, we see some correlations between all tests, but stronger correlations between Exps 1+2, 3+4, respectively

      \end{itemize}

    \subsubsection{Question (ii): Do the existing definitions of at-issueness describe the same underlying phenomenon?}
      Second, by identifying areas of convergence and divergence, we aim to shed light on whether the differing diagnostics reflect distinct theoretical concepts of at-issueness or merely methodological variations in operationalizing a single underlying phenomenon.

    \subsubsection{Differences between tests as evidence for sensitivity to different underlying phenomenon?}

      \begin{itemize}
        \item no obvious distiction between QUD and assertion at issueness
         \item are the differences between tasks because the at-issueness notion that is relevant for questions and assertions is fundamentally different (Koev), or because the diagnostics engage with an underlying notion of QUD-at-issueness, whereas the assertion-based tests introduce some confound(s), as Snider suggests?
      \end{itemize}

    \subsubsection{Correspondence between the tests as evidence for sensitivity to a common underlying phenomenon}
      

      \begin{itemize}
        \item excluding \emph{be right}, we see some correlations between all tests

        \item why is that? -- there must be some common underlying property that they are both sensitive to, even if in different ways, of if there are other confounding factors

        \item for example, snider showed that the specificity of the direct dissent test goes away without polarity particles, suggesting that it is only about anaphoric availabiltiy rather than disagreement. he also argued that the at-issueness of propsitional content is neither necessary not sufficient for their availability as anaphoric antecedents for the propositional anaphor \emph{that}. however, even if anaphoric availability is not THE SAME as at-issuenes, then the correlation suggests that that the dissent-based tests (and therefore maybe anaphoric availability) are also modulated by at-issueness.

      \end{itemize}


    \subsubsection{Differential correspondence between the tests as potential evidence for different underlying phenomena}
      \begin{itemize}
        \item excluding \emph{be right}, we see some correlations between all tests, but stronger correlations between Exps 1+2, 3+4, respectively

        \item there is greater correlation between tests that share theoretical assumptions

        \item while the fact that there is correlation between all of them does suggest that they are sensitive to, or at least influenced by the same underlying phenomenon, there is a difference between the question-based tests and the assertion-based tests, in how much they correlate

      \end{itemize}

     \subsubsection{Different underlying notions?}
      \begin{itemize}
        \item Correlation between Exp. 1 and Exp. 2 is .77, suggesting that higher QUD-match ratings were strongly associated with higher asking-whether ratings.

        \item Correlation between Exp. 3 and Exp. 4 is .77, suggesting that higher accptability ratings in the direct-dissent task were strongly associated with higher proportions of `yes'-answers.

        \item However, the correlation between Exp. 1 and Exps. 3/4, respectively, is only weak to moderate, while the corratlation between Exp. 2 and Exps. 3/4, respectively, would be characterized as strong, but again, is lower than when comparing the question-based and assertion-based diagnostics to each other. Is there any way to quantify this, or do significance testing on that?

        \item The correlation between all diagnostics suggests that there is an underlying property that they all are sensitive to, but the fact that the correlation is higher between those diagnostics that share theoretical assumptions about at-issueness should tell us something too. (what exactly? this is hard to say on an appropriate level of abstraction; I think concluding that they are sensitive to different underlying phenomena would be too strong here)

      \end{itemize}

   

  
  \subsection{Methodological implications \label{ssec:discussion-methodology}}



\section{Conclusion \label{sec:5_conclusion}}
  
  The conclusion is the last numbered section, and any ensuing sections are unnumbered.
  \begin{itemize}
    \item Experimental confirmation for claims that there are empirical differences between at-issueness diagnostics (\citealt{snider_anaphoric_2017,snider_at-issueness_2017,snider_distinguishing_2018,koev_notions_2018,faller_discourse_2019,korotkova_evidential_2020})
      \begin{center}
        \textbf{It matters which diagnostic is used.}\bigskip
      \end{center}


      \item But: difference between QUD at-issueness and assertion-based diagnostics does not seem to be the most important; \textbf{speech act} where the target content is embedded is more important \medskip

      % \item No replication of \citealt{syrett_experimental_2015}: We did not find that final appositives are more at-issue than medial ones

      \item Important question for future work: What the results might tell us about whether the diagnostics reflect a shared underlying notion of at-issueness
  \end{itemize}



\pagebreak
\section*{Abbreviations (if applicable)}\label{abbrev}

  NRRC = non-restrictive relative clause, QUD = question under discussion

\section*{Data availability/Supplementary files (if applicable)}

  The journal encourages authors to make all data associated with their submission openly available, according to the FAIR principles (Findable, Accessible, Interoperable, Reusable). More information can be found \href{https://www.glossa-journal.org/site/editorial-policies/#data-policy}{here}.

  If data/supplementary files are to be associated with the accepted paper, one of the options below should be followed:
  \begin{enumerate}
  \item upload the files to your chosen open repository and make note of the DOI that they will provide (most suitable for datasets or information that act as foundations to the research being published. This option makes the files more findable and more citable). We recommend an open repository such as osf.io, which allows you to create a "project" under which you can upload relevant files (datasets, analysis scripts, experimental materials, etc.). The project will be associated with a unique DOI. You can then include in your manuscript a citation of the OSF entry and/or a link to the project page on OSF, to direct interested readers to the supplementary materials. During review, please be sure that the link to the repository is anonymized to maintain a fully double masked review process. Instructions for doing this on the OSF may be found \href{https://help.osf.io/hc/en-us/articles/360019930333-Create-a-View-only-Link-for-a-Project}{here}. If you'd like to learn more about best practices for ensuring reproducibility, see \href{https://psyarxiv.com/hf297/}{Laurinavichyute and Vasishth (2021)}. Please contact us if you would like more information or advice about hosting your data on an open repository.
  \item upload the files to the journal system during the submission process, as `data files'. The journal will then host them as part of the publication and provide them with a DOI (most suitable for non-data files or very short pieces of information, although option 1 is also suitable for these if the author prefers).
  \end{enumerate}

  \noindent In both cases, a `Data availability' or `Supplementary files' section must be added prior to the reference list that provides a title and very short summary of the files for each file. If option 1 was selected, you should also provide the DOI in this section. For example:

  \noindent Supplementary file 1: Appendix. Scientific data related to the experiments. DOI: \doi{10.5334/gjgl.310.s1}

  Ideally, supplementary files are also cited in the main text.

  Please note that neither of the above two options will result in the files being typeset, so please ensure that they are in publishable format when you upload the accepted paper.


\section*{Ethics and consent (if applicable)}

  Research involving human subjects, human material, or human data, must have been performed in accordance with the Declaration of Helsinki. Studies must have been approved by an appropriate ethics committee and the authors should include a statement in the article text detailing this approval, including the name of the ethics committee and reference number of the approval, or mention any exemptions to ethical approval that apply to their research. The identity of research subjects should be anonymised whenever possible. For research involving human subjects, informed consent to participate in the study must be obtained from participants (or their legal guardian).


\section*{Funding information (if applicable)}

  Should the research have received a funding grant then the grant provider and grant number should be detailed.

\section*{Acknowledgements (optional)}

  The authors wish to thank Martin Haspelmath for providing the generic style sheet for linguistics, and Kai von Fintel for giving permission to use and modify the \textit{Semantics \& Pragmatics} Latex template, bibliography style, and document class.

\section*{Competing interests (required)}

  If any of the authors have any competing interests then these must be declared. Guidelines for competing interests can be found \href{https://www.glossa-journal.org/site/competing-interests/}{here}. If there are no competing interests to declare then the following statement should be present: `The author(s) has/have no competing interests to declare'.

\section*{Authors' contributions (optional)}\label{contrib}

  A sentence or a short paragraph detailing the roles that each author held to contribute to the authorship of the submission.  Individuals listed must fit within the definition of an author, as per our \href{https://www.glossa-journal.org/site/author-guidelines/}{Author Guidelines}.

\nocite{*} %this is to get all the entries of the sample bibliography; delete this line for an actual Glossa submission

%\printbibliography %for use with biblatex; comment out if you use natbib
\bibliography{at-issueness} %for use with natbib; comment out if you use biblatex, and change 'sample' by the name of your bib-file


\appendix

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{ExNo}{0}


\section*{Supplements}

\section{Control stimuli in Exps.~1--4}\label{supp:stims}

  The examples in \ref{control1}-\ref{control4} provide the two control stimuli used in each of Exps.~1-4. For the a.-examples, participants were expected to give a `totally fits' response (Exp.~1), a `yes' response (Exp.~2), a `totally natural' response (Exp.~3), and a `no' response (Exp.~4); for the b.-examples, the opposite response was expected. The numbers  after each example identify the mean ratings (Exps.~1-3) or the proportion of `no' responses (Exp.~4) after excluding participants who did not self-identify as native speakers of American English (but before excluding participants on the basis of these controls), showing that the control stimuli worked as intended.

  \ex.\label{control1} Control stimuli in Exp.~1 (QUD diagnostic)
  \a. Mary: Which course did Ava take?
  \\ John: She took the French course. (.97)
  \b. Jennifer: What does Betsy have?
  \\ Robert: She loves dancing salsa. (.07)

  \ex.\label{control2} Control stimuli in Exp.~2 (`asking whether' diagnostic)
  \a. Mary: Did Arthur take a French course?
  \\ Question to participants: Is Mary asking whether Arthur took a French course? (.96)
  \b. Robert: Does Betsy have a cat?
  \\ Question to participants: Is Robert asking whether Betsy loves apples? (.02)

  \ex.\label{control3} Control stimuli in Exp.~3 (`direct dissent' diagnostic)
  \a. Mary: Arthur took a French course.
  \\ Lily: No, he took a Spanish course. (.87)
  \b. Robert: Betsy has a cat.
  \\ Maximilian: No, she doesn't like apples. (.05)

  \ex.\label{control4} Control stimuli in Exp.~4 (`yes, but' diagnostic)
  \a. Mary: Arthur took a French course.
  \\ Lily: Yes, but Lisa loves cats. / Yes, and he didn't take a French course. / No, he didn't take a French course. (.95)
  \b. Robert: Betsy has a cat.
  \\ Maximilian: Yes, but she is good at math. / Yes, and she loves it so much. / No, she doesn't like apples. (0)

\section{20 clauses}\label{supp:a-clauses}
  The contents of the following 20 clauses, which realized the complements of the 20 clause-embedding predicates, were investigated in Exps.~5--6:

  \begin{multicols}{2}
  \begin{enumerate}%[leftmargin=3ex,itemsep=-2pt]
      \item Mary is pregnant.
      \item Josie went on vacation to France.
      \item Emma studied on Saturday morning.
      \item Olivia sleeps until noon.
      \item Sophia got a tattoo.
      \item Mia drank 2 cocktails last night.
      \item Isabella ate a steak on Sunday.
      \item  Emily bought a car yesterday.
      \item  Grace visited her sister.
      \item Zoe calculated the tip.

  %\columnbrea      
      \item  Danny ate the last cupcake.
      \item  Frank got a cat.
      \item  Jackson ran 10 miles.
      \item  Jayden rented a car.
      \item  Tony had a drink last night.
      \item  Josh learned to ride a bike yesterday.
      \item  Owen shoveled snow last winter.
      \item  Julian dances salsa.
      \item  Jon walks to work.
      \item  Charley speaks Spanish.
          
  \end{enumerate}
  \end{multicols}

\section{Control stimuli in Exps.~5--6}\label{supp:control56}
  The control stimuli in Exps.~5--6 were the contents of the main clause polar questions in \Next. The non-restrictive relative clauses (NRRCs), given in parentheses in \Next, were included in Exp.~6, where at-issueness was measured with an assent diagnostic. The control stimuli here consisted of two clauses (like the target stimuli), to allow the relevant speaker to assent with one of two clauses.

  \ex. Sentences for control stimuli in in question embedding experiments (Exps.~5--6)
    \a. Do these muffins (, which are really delicious,) have blueberries in them?
    \b. Does this pizza (, which I just made from scratch,) have mushrooms on it? 
    \b. Was Jack (, who is my long-time neighbor,) playing outside with the kids? 
    \b. Does Ann (, who is a local performer,) dance ballet?
    \b. Were John's kids (, who are very well-behaved,) in the garage?
    \b. Does Samantha (, who is really into fashion,) have a new hat?
    \z.
  \z.

  We expected participants to give low responses on the at-issueness diagnostics for the control stimuli in \Last, indicating that the main clause content is at-issue.


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: luatex
%%% End:
